defaults:
  - _self_
  - data_cfg: icv_data
  - icv_module: icv_module
  - trainer: zero2
  - lmm: HuggingfaceM4/idefics-9B
  - prompt: ${lmm}_${data_cfg/task}

seed: 426
do_eval: 0
model_cpk_dir: ${oc.env:MODEL_CPK_DIR}
model_name: HuggingFaceM4/idefics-9B
device: "cpu"
# model_name: "HuggingFaceM4/tiny-random-idefics"

result_dir: ${oc.env:RESULT_DIR}

nepochs: 5
n_actions: 16
bs: 8
gc: 8
generate_kwargs:
  max_new_tokens: 15

nproc_per_node: 1
nnodes: 1
node_rank: 0
script_path: "agent_ppoLoRA/main_grpo.py"
deepspeed_config: "/scratch/s223540177/LIVE-Learnable-In-Context-Vector/open_r1_multimodal/local_scripts/zero3.json"
output_dir: "/scratch/s223540177/LIVE-Learnable-In-Context-Vector/checkpoints/HuggingFaceM4/idefics-9b/model_grpo.pth"
model_name_or_path: "HuggingFaceM4/idefics-9b"
dataset_name: "vqav2"
max_prompt_length: 8192
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
logging_steps: 1
bf16: True
# report_to: "wandb"
gradient_checkpointing: True
attn_implementation: "flash_attention_2"
min_pixels: 1
max_pixels: 2359296
save_total_limit: 8
# num_train_epochs: 1
run_name: "HuggingFaceM4/idefics-9b"
use_peft: False



hydra:
  run:
    dir: ${result_dir}/hydra_output/${hydra.job.name}/${run_name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: ${result_dir}/hydra_output/${hydra.job.name}/${run_name}/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.override_dirname}
